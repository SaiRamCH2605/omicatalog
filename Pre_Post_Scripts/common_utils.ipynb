{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413553b4-881c-4760-93b1-f4102c9fe8c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-06 04:05:34,715 [INFO] -> [Thread-6] - wait_for_commands - 575 - Received command c on object id p0 \n"
     ]
    }
   ],
   "source": [
    "import logging as log\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81283680-e115-4830-b7ff-5b6cf413a3a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_log_level(level):\n",
    "    level = level.lower()\n",
    "    if  level == \"info\":\n",
    "        return log.INFO\n",
    "    elif level == \"debug\":\n",
    "        return log.DEBUG\n",
    "    elif level == \"trace\":\n",
    "        return log.CRITICAL\n",
    "\n",
    "# initialize log\n",
    "def init_log(config_data):\n",
    "    try:\n",
    "        log_format = f\"%(asctime)s [%(levelname)s] -> [%(threadName)s] - %(funcName)s - %(lineno)d - %(message)s \"\n",
    "        log_formatter = log.Formatter(log_format)\n",
    "\n",
    "        root_logger = log.getLogger()\n",
    "        root_logger.setLevel(get_log_level(config_data.get('logging').get('mode')))\n",
    "        if root_logger.handlers:\n",
    "            root_logger.handlers.clear()\n",
    "\n",
    "        console_handler = log.StreamHandler()\n",
    "        console_handler.setLevel(get_log_level(config_data.get('logging').get('mode')))\n",
    "        console_handler.setFormatter(log_formatter)\n",
    "        root_logger.addHandler(console_handler)\n",
    "\n",
    "        file_date = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        log_dir = config_data.get('logging').get('folder_path')\n",
    "        log_file_name = f\"/{config_data.get('logging').get('file_name')}_{file_date}.log\"\n",
    "        log_path = f\"{log_dir}{log_file_name}\"\n",
    "        log.info(log_path)\n",
    "#         log.basicConfig(level=log.INFO, filename=log_path, filemode=\"a\",\n",
    "#                         format=log_format)\n",
    "        \n",
    "        file_handler = log.FileHandler(log_path,mode='a')\n",
    "        file_handler.setFormatter(log_formatter)\n",
    "        root_logger.addHandler(file_handler)\n",
    "    except Exception as e:\n",
    "        raise Exception('log initialization failed: %s' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc2feb7-a351-4377-9fbd-a2b6ccac51de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-06 04:05:34,817 [INFO] -> [Thread-6] - wait_for_commands - 575 - Received command c on object id p0 \n"
     ]
    }
   ],
   "source": [
    "# initialize configuration\n",
    "def init_config(config_path):\n",
    "    try:\n",
    "        \"\"\"read configuration from json fle\"\"\"\n",
    "        config_data = spark.read.option(\"multiline\",\"true\").json(config_path).collect()[0]\n",
    "        config_data = config_data.asDict(True)\n",
    "#         \"\"\"hardcoded\"\"\"\n",
    "#         config_data = {'job_num': {'ods_job_num': 3}, 'logging': {'file_name': 'parquet', 'folder_path': '/tmp', 'mode': 'info'}, 'source': {'catalog_nm':'`omi-catalog`','audit_schema_nm': '`omi-catalog`.si_core_audit', 'jdbc_offset': 100000, 'database': 'postgres', 'jdbc_fetch_size': 10000,'delta_partitions': 10,'jdbc_partitions': 10, 'driver': 'org.postgresql.Driver', 'password': 'Sapiens2022', 'url': 'jdbc:postgresql://omi-tia-ods-perftest-cluster.cluster-cq2voeb9fipc.eu-west-1.rds.amazonaws.com:5432/tiadr', 'user': 'postgres'}}\n",
    "        init_log(config_data)\n",
    "        return config_data\n",
    "    except Exception as e:\n",
    "        raise Exception('config initialization failed: %s' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16801143-849b-4b59-9060-ac95fe453cb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get dataframe from relational db using jdbc connection\n",
    "def get_df(config_data,query):\n",
    "    try:\n",
    "#         dataframe = spark.read.jdbc(url=jdbcUrl, table=query, properties=jdbcProperties)\n",
    "        dataframe = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", config_data.get('source').get('url')) \\\n",
    "                .option(\"driver\", config_data.get('source').get('driver')) \\\n",
    "                .option(\"query\", query) \\\n",
    "                .option(\"user\", config_data.get('source').get('user')) \\\n",
    "                .option(\"password\", config_data.get('source').get('password')) \\\n",
    "                .option(\"fetchsize\",config_data.get('source').get('jdbc_fetch_size')) \\\n",
    "                .load()\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        print('exception in get_df: %s' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847509a8-5454-4943-9082-4de528dc743c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-06 04:05:35,018 [INFO] -> [Thread-6] - wait_for_commands - 575 - Received command c on object id p0 \n"
     ]
    }
   ],
   "source": [
    "def exec_spark_sql(query,config_data=None):\n",
    "    try:\n",
    "        df = spark.sql(query)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f'exception in exec_spark_sql: {e} : query: {query}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_utils",
   "notebookOrigID": 3710461963871531,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
