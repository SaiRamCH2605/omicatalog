{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611831e8-6c96-4c26-9606-07f2ce2a6902",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": ""
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "eff75c6a-ec7e07fe2941a91c37641658"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b659b954-cbc3-41e5-81c7-84249ee1abf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def exec_spark_sql(query,config_data=None):\n",
    "    try:\n",
    "        df = spark.sql(query)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f'exception in exec_spark_sql: {e} : query: {query}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01df409a-776c-43e7-a85f-5d6fecd5cf89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pre and post script auditing status description\n",
    "def get_status_desc(status_num):\n",
    "    status_desc = {\n",
    "        1: \"'STARTED'\",\n",
    "        2: \"'ERROR_ENCOUNTERED'\",\n",
    "        3: \"'COMPLETED'\",\n",
    "        4: \"'ACTIVE'\",\n",
    "        5: \"'INACTIVE'\"\n",
    "    } \n",
    "    return status_desc.get(status_num, \"'nothing'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853ebc72-7841-4109-8a28-1a497e1edac6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# initialize job level prescript\n",
    "def job_pre_script(config_data,job_num,current_batch_start_date):\n",
    "    # status_num = 1 indicated job is started\n",
    "    status_num = 1\n",
    "    # get batch_id / sub_batch_id / previous_batch_start_date / previous_batch_end_date\n",
    "    pre_script_query = f\"\"\"\n",
    "        select * from\n",
    "        (select case\n",
    "        when MAX (BATCH_ID) is null then 1\n",
    "        when DATEDIFF ( current_timestamp() ,current_batch_start_date ) = 0 then MAX (BATCH_ID)\n",
    "        when DATEDIFF ( current_timestamp() ,current_batch_start_date ) = -1 then MAX (BATCH_ID)\n",
    "        when DATEDIFF ( current_timestamp() ,current_batch_start_date ) > 0 then MAX (BATCH_ID) + 1\n",
    "        end as batch_id from {config_data.get('source').get('audit_schema_nm')}.JOB_DETAILS where JOB_NUM = {job_num}\n",
    "        group by CURRENT_BATCH_START_DATE ) a\n",
    "        cross join \n",
    "        (select\n",
    "        case when MAX (SUB_BATCH_ID) is null then 1\n",
    "        else MAX(sub_batch_id)+ 1 end as sub_batch_id\n",
    "        from {config_data.get('source').get('audit_schema_nm')}.JOB_DETAILS where\n",
    "        DATEDIFF ( current_timestamp() ,current_batch_start_date ) = 0\n",
    "        or DATEDIFF ( current_timestamp() ,current_batch_start_date ) = -1 and JOB_NUM = {job_num}\n",
    "        group by CURRENT_BATCH_START_DATE) b\n",
    "        cross join\n",
    "        (select current_batch_start_date from {config_data.get('source').get('audit_schema_nm')}.JOB_DETAILS where job_num = {job_num}) c\n",
    "        cross join\n",
    "        (select current_batch_end_date from {config_data.get('source').get('audit_schema_nm')}.JOB_DETAILS where job_num = {job_num}) d\n",
    "    \"\"\"\n",
    "    pre_script_df = exec_spark_sql(pre_script_query).toPandas().to_dict('records')\n",
    "    if len(pre_script_df) == 0 :\n",
    "        data = {'batch_id': 1,'sub_batch_id' : 1, 'current_batch_start_date': pd.NaT,'current_batch_end_date': pd.NaT}\n",
    "        pre_script_df.append(data)\n",
    "    batch_id = pre_script_df[0].get('batch_id')\n",
    "    sub_batch_id = pre_script_df[0].get('sub_batch_id')\n",
    "    previous_batch_start_date = pre_script_df[0].get('current_batch_start_date')\n",
    "    previous_batch_end_date = pre_script_df[0].get('current_batch_end_date')\n",
    "    previous_batch_start_date = f\"'{previous_batch_start_date}'\" if not pd.isnull(previous_batch_start_date) else \"'1800-01-01'\"\n",
    "    previous_batch_end_date = f\"'{previous_batch_end_date}'\" if not pd.isnull(previous_batch_end_date) else \"null\"\n",
    "    \n",
    "    # merge into job_details table as only one entry should be present for each job\n",
    "    job_details_query = f\"\"\"\n",
    "    merge INTO {config_data.get('source').get('audit_schema_nm')}.JOB_DETAILS t \n",
    "    using (\n",
    "        select \n",
    "        {job_num} job_id,\n",
    "        {batch_id} batch_id,\n",
    "        {sub_batch_id} sub_batch_id,\n",
    "        {job_num} job_num,\n",
    "        null core_db_batch_start_date,\n",
    "        null core_db_batch_end_date,\n",
    "        '{current_batch_start_date}' current_batch_start_date,\n",
    "        null current_batch_end_date,\n",
    "        {previous_batch_start_date} previous_batch_start_date,\n",
    "        {previous_batch_end_date} previous_batch_end_date,\n",
    "        {status_num} status_num,\n",
    "        null failure_point,\n",
    "        null technical_error_code,\n",
    "        {get_status_desc(status_num)} status_desc,\n",
    "        null technical_error_desc\n",
    "    ) s\n",
    "    ON s.job_num = t.job_num\n",
    "    WHEN MATCHED THEN UPDATE SET \n",
    "        job_id = s.job_id,\n",
    "        batch_id= s.batch_id,\n",
    "        sub_batch_id= s.sub_batch_id,\n",
    "        job_num= s.job_num,\n",
    "        core_db_batch_start_date= s.core_db_batch_start_date,\n",
    "        core_db_batch_end_date= s.core_db_batch_end_date,\n",
    "        current_batch_start_date= s.current_batch_start_date,\n",
    "        current_batch_end_date= s.current_batch_end_date,\n",
    "        previous_batch_start_date= s.previous_batch_start_date,\n",
    "        previous_batch_end_date= s.previous_batch_end_date,\n",
    "        status_num= s.status_num,\n",
    "        failure_point= s.failure_point,\n",
    "        technical_error_code= s.technical_error_code,\n",
    "        status_desc= s.status_desc,\n",
    "        technical_error_desc= s.technical_error_desc\n",
    "        WHEN NOT MATCHED THEN INSERT (\n",
    "            job_id,\n",
    "            batch_id,\n",
    "            sub_batch_id,\n",
    "            job_num,\n",
    "            core_db_batch_start_date,\n",
    "            core_db_batch_end_date,\n",
    "            current_batch_start_date,\n",
    "            current_batch_end_date,\n",
    "            previous_batch_start_date,\n",
    "            previous_batch_end_date,\n",
    "            status_num,\n",
    "            failure_point,\n",
    "            technical_error_code,\n",
    "            status_desc,\n",
    "            technical_error_desc\n",
    "          )\n",
    "      VALUES (\n",
    "            s.job_id,\n",
    "            s.batch_id,\n",
    "            s.sub_batch_id,\n",
    "            s.job_num,\n",
    "            s.core_db_batch_start_date,\n",
    "            s.core_db_batch_end_date,\n",
    "            s.current_batch_start_date,\n",
    "            s.current_batch_end_date,\n",
    "            s.previous_batch_start_date,\n",
    "            s.previous_batch_end_date,\n",
    "            s.status_num,\n",
    "            s.failure_point,\n",
    "            s.technical_error_code,\n",
    "            s.status_desc,\n",
    "            s.technical_error_desc\n",
    "          )\n",
    "    \"\"\"\n",
    "    exec_spark_sql(job_details_query)\n",
    "    \n",
    "    # create history of job_details for each job run\n",
    "    job_details_history_query = f\"insert into {config_data.get('source').get('audit_schema_nm')}.JOB_DETAILS_HISTORY (job_id,batch_id,sub_batch_id,job_num,core_db_batch_start_date,core_db_batch_end_date,current_batch_start_date,current_batch_end_date,previous_batch_start_date,previous_batch_end_date,status_num,failure_point,technical_error_code,status_desc,technical_error_desc) values({job_num},{batch_id},{sub_batch_id},{job_num},null,null,'{current_batch_start_date}',null,{previous_batch_start_date},{previous_batch_end_date},{status_num},null,null,{get_status_desc(status_num)},null)\"\n",
    "    exec_spark_sql(job_details_history_query)\n",
    "    \n",
    "    batch_sub_batch_id = {'batch_id': batch_id,'sub_batch_id' : sub_batch_id}\n",
    "    return batch_sub_batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9eb95d-37e0-4b2e-bd2a-5ab1f96bba22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# job level postscript - if job is completed or failed\n",
    "def job_post_script(config_data,job_num,batch_sub_batch_id,status_num,current_batch_end_date=\"null\",failure_point=\"null\",technical_error_code=\"null\",technical_error_desc=\"null\"):\n",
    "    job_details_post_script_query = f\"\"\"\n",
    "        update {config_data.get('source').get('audit_schema_nm')}.JOB_DETAILS \n",
    "        set STATUS_NUM =  {status_num} , CURRENT_BATCH_END_DATE= '{current_batch_end_date}' ,  \n",
    "        Status_Desc= {get_status_desc(status_num)}, failure_point = {failure_point} , technical_error_code = {technical_error_code} , technical_error_desc = {technical_error_desc}\n",
    "        where JOB_NUM= {job_num}  AND BATCH_ID= {batch_sub_batch_id.get('batch_id')}  AND \n",
    "        SUB_BATCH_ID= {batch_sub_batch_id.get('sub_batch_id')} \n",
    "       \"\"\"\n",
    "    job_details_history_post_script_query = f\"\"\"\n",
    "        update {config_data.get('source').get('audit_schema_nm')}.JOB_DETAILS_History \n",
    "        set STATUS_NUM = {status_num} ,  CURRENT_BATCH_END_DATE= '{current_batch_end_date}' ,\n",
    "        Status_Desc= {get_status_desc(status_num)} , failure_point = {failure_point} , technical_error_code = {technical_error_code} , technical_error_desc = {technical_error_desc}\n",
    "        where JOB_NUM= {job_num}  AND BATCH_ID=  {batch_sub_batch_id.get('batch_id')}  AND \n",
    "        SUB_BATCH_ID= {batch_sub_batch_id.get('sub_batch_id')} \n",
    "    \"\"\"\n",
    "    log.debug(f\"job_details_post_script_query: {job_details_post_script_query}\")\n",
    "    log.debug(f\"job_details_history_post_script_query: {job_details_history_post_script_query}\")\n",
    "    exec_spark_sql(job_details_post_script_query)\n",
    "    exec_spark_sql(job_details_history_post_script_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c88dcfc-5575-466e-b3c7-f75f6cb4126a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Retrieve the values of the passed parameters\n",
    "# function_name = dbutils.widgets.get(\"function_name\")\n",
    "# config_data = dbutils.widgets.get(\"config_data\")\n",
    "# job_num = dbutils.widgets.get(\"job_num\")\n",
    "# current_batch_start_date = dbutils.widgets.get(\"current_batch_start_date\")\n",
    "\n",
    "# batch_sub_batch_id = dbutils.widgets.get(\"batch_sub_batch_id\")\n",
    "# status_num = dbutils.widgets.get(\"status_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "effdad6f-bf71-4e8f-8110-1ac6b90a14b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Call the function based on the function_name parameter\n",
    "# if function_name == \"job_pre_script\":\n",
    "#     return job_pre_script(config_data,job_num,current_batch_start_date)\n",
    "\n",
    "# # Call the function based on the function_name parameter\n",
    "# if function_name == \"job_post_script\":\n",
    "#     return job_post_script(config_data,job_num,batch_sub_batch_id,status_num)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "job_level_scripting",
   "notebookOrigID": 122484843534518,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
